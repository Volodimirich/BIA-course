{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from scipy.signal import find_peaks,lfilter, butter, welch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function performs bandpass filter for the digital signal \n",
    "\n",
    "Parameters: \n",
    "-----------\n",
    "data: array_like\n",
    "    An N-dimensional input array\n",
    "lowcut: float\n",
    "    Low cutoff frequency\n",
    "highcut: float\n",
    "    High cutoff frequency\n",
    "fs: float\n",
    "    Sampling rate of the signal\n",
    "order: int\n",
    "    The order of the filter.\n",
    "\n",
    "Returns: \n",
    "-------\n",
    "filtered_signal: array\n",
    "    The output of the digital filter\n",
    "\"\"\"\n",
    "\n",
    "def butter_bandpass(data, lowcut, highcut, fs, order=2):\n",
    "    nyq = 0.5 * fs\n",
    "    \n",
    "    high = highcut / nyq\n",
    "    low = lowcut / nyq\n",
    "    \n",
    "    b, a = butter(order, [low, high], btype='bandpass', analog=False)\n",
    "    filtered_signal = lfilter(b, a, data)\n",
    "    \n",
    "    return filtered_signal\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Fill NaN with zeros\n",
    "\n",
    "Parameters: \n",
    "-----------\n",
    "df: DataFrame\n",
    "    DataFrame with NaNs\n",
    "    \n",
    "Returns: \n",
    "-------\n",
    "df: Dataframe\n",
    "    All NaNs filled by zeros\n",
    "\"\"\"\n",
    "\n",
    "def del_nul_and_nan(df):\n",
    "    \n",
    "    df.fillna(value=0, axis=1, inplace=True)    \n",
    "    return df\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "Parameters: \n",
    "-----------\n",
    "df: pd.DataFrame\n",
    "    An N-dimensional input DataFrame\n",
    "init_target: pd.DataFrame\n",
    "    targets of df\n",
    "lowcut: float\n",
    "    Low cutoff frequency\n",
    "highcut: float\n",
    "    High cutoff frequency\n",
    "fs: float\n",
    "    Sampling rate of the signal\n",
    "flag_number: float\n",
    "    special value which is unique for each recording (it should be the same for all windows from one recording)\n",
    "\n",
    "Returns: \n",
    "-------\n",
    "fft_out: pd.DataFrame\n",
    "    An output Dataframe with spectrum power and frequencies\n",
    "final_targets: pd.DataFrame\n",
    "    An output DataFrame with all targets for the file\n",
    "multiple_flags: pd.DataFrame\n",
    "    An output DataFrame with all flags for the file\n",
    "\"\"\"\n",
    "\n",
    "def spec_and_freq_for_single_df(df, init_target, fs, flag_number, lowcut, highcut):\n",
    "    \n",
    "    fft_out = pd.DataFrame()\n",
    "    final_targets = pd.DataFrame()\n",
    "    final_target = []\n",
    "    multiple_flag=[]\n",
    "    target_raw = init_target.iloc[0]\n",
    "    duration=5 #time duration of window in sec\n",
    "    step = 500 #step of window beginning in msec\n",
    "    t0=0 #start time\n",
    "    end_point = int(t0+duration*fs)\n",
    "    i=1 #counter\n",
    "    while end_point < df.shape[0]:\n",
    "        flag= flag_number\n",
    "        for col in range(df.shape[1]):\n",
    "            target = target_raw.iloc[col]\n",
    "            end_point = int(t0+duration*fs)\n",
    "            N = int(duration*fs)\n",
    "            interesting_df = df.iloc[t0:end_point,col]\n",
    "            filtered_yf = butter_bandpass(interesting_df, lowcut, highcut, fs, order=2)\n",
    "            #print(col,t0,i)\n",
    "\n",
    "            fft_yf = np.fft.fft(filtered_yf) #spectrum\n",
    "            fft_xf = np.fft.fftfreq(N, 1/fs) #frequencies\n",
    "\n",
    "            fft_20_index = np.argwhere((fft_xf<20) & (fft_xf>0))        \n",
    "            fft_yf_20 = fft_yf[fft_20_index] #cutting on 20Hz\n",
    "            fft_xf_20 = fft_xf[fft_20_index] #cutting on 20Hz\n",
    "\n",
    "            fft_yf_20 = pd.DataFrame(np.abs(fft_yf_20) / N, columns=[df.keys()[col] + '_' + str(i) + '_yf'])\n",
    "            fft_xf_20 = pd.DataFrame(fft_xf_20, columns=[df.keys()[col] + '_' + str(i) + '_xf']) \n",
    "\n",
    "            fft_out = pd.concat([fft_out, fft_yf_20, fft_xf_20], axis=1)\n",
    "            final_target.append(target)\n",
    "            multiple_flag.append(flag)\n",
    "            flag += 1\n",
    "        t0 = t0 + step\n",
    "        i=i+1\n",
    "    final_targets = pd.DataFrame(final_target,columns=['target'])\n",
    "    multiple_flags = pd.DataFrame(multiple_flag,columns=['flag'])    \n",
    "    return(fft_out, final_targets,multiple_flags)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Creating DataFrame of target-flag combination \n",
    "\n",
    "Parameters: \n",
    "-----------\n",
    "init_target: DataFrame\n",
    "    Targets of initial dataset\n",
    "flag_number: int\n",
    "    Old number of flag from previous iteration\n",
    "Returns: \n",
    "-------\n",
    "target_flag: DataFrame\n",
    "    combination of target-flag couples\n",
    "flag_number: int\n",
    "    the last number of flag after passing current file\n",
    "\"\"\"\n",
    "\n",
    "def target_flag_combination(init_target, flag_number):\n",
    "    final_flag=[]\n",
    "    final_target=[]\n",
    "    target_raw = init_target.iloc[0]\n",
    "    for col in range(init_target.shape[1]):\n",
    "        target = target_raw.iloc[col]\n",
    "        final_target.append(target)\n",
    "        final_flag.append(flag_number)\n",
    "        flag_number += 1\n",
    "    final_targets = pd.DataFrame(final_target,columns=['target'])\n",
    "    final_flags = pd.DataFrame(final_flag,columns=['flag'])\n",
    "    target_flag = pd.concat([final_targets, final_flags], axis=1)  \n",
    "    return(target_flag, flag_number)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Creating the Fourier spectra of list of files\n",
    "\n",
    "Parameters: \n",
    "-----------\n",
    "df: list\n",
    "    List with DataFrames of simultaneous OM (Optical) and EM (Electrode) Mapping\n",
    "flag_number: float\n",
    "    special value which is unique for each recording (it should be the same for all windows from one recording)   \n",
    "Returns: \n",
    "-------\n",
    "all_fft_el, all_fft_om: DataFrames\n",
    "    Lists with spectrum and frequencies DataFrames\n",
    "all_targets_el, all_targets_om: DataFrames\n",
    "    Lists with targets for EM and OM DataFrames\n",
    "all_flags_el, all_flags_om: DataFrames\n",
    "    Lists with targets for EM and OM DataFrames\n",
    "final_target_flag: DataFrame\n",
    "    DataFrame with combined targets and flags\n",
    "flag_number: int\n",
    "    the last number of flag after passing all files\n",
    "\"\"\"\n",
    "\n",
    "def full_spec_and_freq(df, flag_number):\n",
    "    \n",
    "    all_fft_el = pd.DataFrame()\n",
    "    all_fft_om = pd.DataFrame()\n",
    "    all_targets_el = pd.DataFrame()\n",
    "    all_targets_om = pd.DataFrame()\n",
    "    all_flags_el = pd.DataFrame()\n",
    "    all_flags_om = pd.DataFrame()\n",
    "    \n",
    "    electrode_signal = df[df.columns[::3]]\n",
    "    optical_signal = df[df.columns[2::3]]\n",
    "    init_target = df[df.columns[1::3]]\n",
    "\n",
    "    electrode_signal = del_nul_and_nan(electrode_signal)\n",
    "    optical_signal = del_nul_and_nan(optical_signal)        \n",
    "\n",
    "    Fs_el = 1017.25 # sampling rate\n",
    "    Fs_om = 1000.0    \n",
    "    fft_el, target_el, flags_el = spec_and_freq_for_single_df(electrode_signal, init_target, Fs_el, flag_number, lowcut=2, highcut=20.0)\n",
    "    fft_om, target_om, flags_om = spec_and_freq_for_single_df(optical_signal, init_target, Fs_om, flag_number, lowcut=2, highcut=20.0)\n",
    "    final_target_flag, flag_number = target_flag_combination(init_target, flag_number)\n",
    "                    \n",
    "    all_fft_el = pd.concat([all_fft_el, fft_el], axis=1)\n",
    "    all_fft_om = pd.concat([all_fft_om, fft_om], axis=1)\n",
    "    all_targets_el = pd.concat([all_targets_el, target_el], axis=1)\n",
    "    all_targets_om = pd.concat([all_targets_om, target_om], axis=1) \n",
    "    all_flags_el = pd.concat([all_flags_el, flags_el], axis=1)\n",
    "    all_flags_om = pd.concat([all_flags_om, flags_om], axis=1)   \n",
    "    return(all_fft_el, all_fft_om, all_targets_el, all_targets_om, all_flags_el, all_flags_om,final_target_flag, flag_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function that generates pd.DataFrame with different properties for n highest peaks. Properties are values of frequency, \n",
    "height, width, prominence. \n",
    "\n",
    "For more details about properties - https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.find_peaks.html\n",
    "\n",
    "Parameters: \n",
    "\n",
    "full_df: Dataframe\n",
    "Dataframe with fourier spectrum\n",
    "\n",
    "n: int\n",
    "number of peaks\n",
    "\n",
    "lf_th: float\n",
    "threshold for low frequency noise (1Hz by default)\n",
    "\n",
    "Returns: \n",
    "\n",
    "features: DataFrame, shape=(full_df.shape[1]/2, n*4)\n",
    "properties of n hightest peaks\n",
    "\"\"\"\n",
    "def properties_of_peaks(list_df, n, lf_th=1): \n",
    "    index = []\n",
    "    freq = []\n",
    "    height = []\n",
    "    width = []\n",
    "    prominence = []\n",
    "    prom_ratio=[]\n",
    "    height_ratio=[]\n",
    "    snr = []\n",
    "    ratio_columns = ['1 and 2', '1 and 3', '1 and 4', '2 and 3', '2 and 4', '3 and 4']\n",
    "    \n",
    "    features = pd.DataFrame()\n",
    "    lengh = 0\n",
    "    for k in range(len(list_df)):\n",
    "        full_df = list_df[k]\n",
    "        lengh = lengh + full_df.shape[1]\n",
    "        df = full_df[full_df.columns[::2]][:-1]\n",
    "        xf = full_df[full_df.columns[1::2]][:-1]\n",
    "        all_peaks = []\n",
    "        all_props = []\n",
    "        for col in df:\n",
    "            peaks, properties = find_peaks(df[col], height=0, width=0, prominence=0, rel_height=0.5)\n",
    "            properties['peak_index'] = peaks\n",
    "            all_props.append(properties)\n",
    "            all_peaks.append(peaks)\n",
    "\n",
    "        for i in range(len(all_props)):\n",
    "            z = np.argsort(all_props[i]['peak_heights'])\n",
    "            z = z[:-(n+1):-1]\n",
    "            if len(z) < 5:\n",
    "                zeros_raw_n=np.zeros(n)\n",
    "                height.append(zeros_raw_n)\n",
    "                width.append(zeros_raw_n)\n",
    "                prominence.append(zeros_raw_n)\n",
    "                ratio_height = []\n",
    "                ratio_prominence = []\n",
    "                ratio_height = np.zeros(n+1)\n",
    "                ratio_prominence = np.zeros(n+1)\n",
    "                height_ratio.append(ratio_height)\n",
    "                prom_ratio.append(ratio_prominence)\n",
    "                for j in range(n):\n",
    "                    fr = np.zeros(1)\n",
    "                    freq.append(fr)                \n",
    "            else:       \n",
    "                idx = all_props[i]['peak_index'][z] #index of max peaks\n",
    "                for j in range(n):\n",
    "                    fr = xf.iloc[:,i][all_peaks[i][z][j]] #freqs of max peaks\n",
    "                    freq.append(fr)\n",
    "\n",
    "                h = all_props[i]['peak_heights'][z] #heights of max peaks\n",
    "                height.append(h)\n",
    "                ratio_height = []\n",
    "                ratio_height = np.append(ratio_height, h[0]/h[1]) #ratio between height of 1st highest peak and 2nd\n",
    "                ratio_height = np.append(ratio_height, h[0]/h[2]) #ratio between height of 1st highest peak and 3rd\n",
    "                ratio_height = np.append(ratio_height, h[0]/h[3]) #ratio between height of 1st highest peak and 4th\n",
    "                ratio_height = np.append(ratio_height, h[1]/h[2]) #ratio between height of 2nd highest peak and 3rd\n",
    "                ratio_height = np.append(ratio_height, h[1]/h[3]) #ratio between height of 2nd highest peak and 4th\n",
    "                ratio_height = np.append(ratio_height, h[2]/h[3]) #ratio between height of 3rd highest peak and 4th\n",
    "                height_ratio.append(ratio_height)\n",
    "                w = all_props[i]['widths'][z] #width of max peaks\n",
    "                width.append(w)\n",
    "                p = all_props[i]['prominences'][z]\n",
    "                prominence.append(p)\n",
    "                ratio_prominence = []\n",
    "                ratio_prominence = np.append(ratio_prominence,p[0]/p[1]) #ratio between prominence of 1st highest peak and 2nd\n",
    "                ratio_prominence = np.append(ratio_prominence,p[0]/p[2]) #ratio between prominence of 1st highest peak and 3rd\n",
    "                ratio_prominence = np.append(ratio_prominence,p[0]/p[3]) #ratio between prominence of 1st highest peak and 4th\n",
    "                ratio_prominence = np.append(ratio_prominence,p[1]/p[2]) #ratio between prominence of 2nd highest peak and 3rd\n",
    "                ratio_prominence = np.append(ratio_prominence,p[1]/p[3]) #ratio between prominence of 2nd highest peak and 4th\n",
    "                ratio_prominence = np.append(ratio_prominence,p[2]/p[3]) #ratio between prominence of 3rd highest peak and 4th\n",
    "                prom_ratio.append(ratio_prominence)\n",
    "           \n",
    "\n",
    "        y_col = [col for col in df.columns]\n",
    "        y_df = df[y_col]\n",
    "\n",
    "        for i in range(y_df.shape[1]):          \n",
    "            s = y_df[y_col[i]][y_df[y_col[i]] != 0]\n",
    "\n",
    "            _, properties = find_peaks(s, height=0)\n",
    "            mean_max = np.mean(np.sort(properties['peak_heights'])[-2:])#np.max(properties['peak_heights'])\n",
    "            sd = s.std(axis=0)\n",
    "            if np.isnan(mean_max)==True:\n",
    "                ratio = 0\n",
    "            else:\n",
    "                ratio = np.round(mean_max / sd, 2)\n",
    "                ratio = np.where(sd == 0, 0, ratio)\n",
    "            snr.append(ratio)\n",
    "        \n",
    "    snr = pd.DataFrame(snr, columns=['SNR'])        \n",
    "    freq = np.reshape(freq, ((int(lengh/2)), n)) \n",
    "    freq = pd.DataFrame(freq, columns=['freq ' + str(i) for i in range(n)])\n",
    "    height = pd.DataFrame(height, columns=['height ' + str(i) for i in range(n)])\n",
    "    width = pd.DataFrame(width, columns=['width ' + str(i) for i in range(n)])\n",
    "    prominence = pd.DataFrame(prominence, columns=['prominence ' + str(i) for i in range(n)])\n",
    "    height_ratio = pd.DataFrame(height_ratio, columns=['height ratio between peaks '+ str(i) for i in ratio_columns])\n",
    "    prom_ratio = pd.DataFrame(prom_ratio, columns=['prominence ratio between peaks '+ str(i) for i in ratio_columns])\n",
    "    \n",
    "    features = pd.concat([features, freq, height, width, prominence, height_ratio, prom_ratio,snr], axis=1)\n",
    "    \n",
    "    return(features)\n",
    "\n",
    "\"\"\"\n",
    "Function that generates pd.DataFrame with amount of peaks for (th*100)% threshhold. \n",
    "\n",
    "Parameters: \n",
    "\n",
    "full_df: Dataframe\n",
    "Dataframe with fourier spectrum\n",
    "\n",
    "th: float, from 0 to 1\n",
    "threshhold\n",
    "\n",
    "Returns: \n",
    "\n",
    "features: DataFrame, shape=(full_df[1]/2, 1)\n",
    "number of peaks\n",
    "\"\"\"\n",
    "\n",
    "def number_of_peaks(list_df, th):\n",
    "    num_of_peaks = []\n",
    "    for k in range(len(list_df)):\n",
    "        full_df = list_df[k]\n",
    "        all_props = []\n",
    "        df = full_df[full_df.columns[::2]][:-1]\n",
    "        for col in df:\n",
    "            _, properties = find_peaks(df[col], height=0)\n",
    "            all_props.append(properties)\n",
    "\n",
    "        for i in range(len(all_props)):\n",
    "            if len(all_props[i]['peak_heights'])< 5:\n",
    "                num =0\n",
    "            else:\n",
    "                max_height = np.max(all_props[i]['peak_heights'])\n",
    "                peaks, _ = find_peaks(df.iloc[:,i], threshold=th*max_height)\n",
    "                num = peaks.shape[0]\n",
    "            num_of_peaks.append(num)\n",
    "    num_of_peaks = pd.DataFrame(num_of_peaks, columns=['#peaks_' + str(th)])\n",
    "    return(num_of_peaks)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Function that generates list with calculated neighbor features with kernel 3 by 3\n",
    "\n",
    "Parameters: \n",
    "\n",
    "previous_grid: list of shape (8,8)\n",
    "Grid of one feature\n",
    "\n",
    "Returns: \n",
    "\n",
    "final_grid: list of shape (8,8)\n",
    "Calculated neighbor grid of one feature\n",
    "\"\"\"\n",
    "def calculation_neighbour_features(previous_grid):\n",
    "    final_grid=[]\n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            if i ==0:\n",
    "                if j==0:\n",
    "                    final_grid.append(float((previous_grid[i+1,j]+previous_grid[i,j+1]+previous_grid[i+1,j+1])/3))\n",
    "                elif j==7:\n",
    "                    final_grid.append(float((previous_grid[i+1,j]+previous_grid[i,j-1]+previous_grid[i+1,j-1])/3))\n",
    "                else:\n",
    "                    final_grid.append(float((previous_grid[i+1,j]+previous_grid[i,j+1]+previous_grid[i+1,j+1]+previous_grid[i,j-1]+previous_grid[i+1,j-1])/5))\n",
    "            if i != 0 and i!=7:\n",
    "                if j==0:\n",
    "                    final_grid.append(float((previous_grid[i+1,j]+previous_grid[i-1,j]+previous_grid[i+1,j+1]+previous_grid[i,j+1]+previous_grid[i-1,j+1])/5))\n",
    "                elif j==7:\n",
    "                    final_grid.append(float((previous_grid[i+1,j]+previous_grid[i-1,j]+previous_grid[i+1,j-1]+previous_grid[i,j-1]+previous_grid[i-1,j-1])/5))\n",
    "                else:\n",
    "                    final_grid.append(float((previous_grid[i,j-1]+previous_grid[i,j+1]+previous_grid[i+1,j]+previous_grid[i+1,j-1]+previous_grid[i+1,j+1]+previous_grid[i-1,j]+previous_grid[i-1,j-1]+previous_grid[i-1,j+1])/8))\n",
    "            if i ==7:\n",
    "                if j==0:\n",
    "                    final_grid.append(float((previous_grid[i-1,j]+previous_grid[i,j+1]+previous_grid[i-1,j+1])/3))\n",
    "                elif j==7:\n",
    "                    final_grid.append(float((previous_grid[i-1,j]+previous_grid[i,j-1]+previous_grid[i-1,j-1])/3))\n",
    "                else:\n",
    "                    final_grid.append(float((previous_grid[i,j-1]+previous_grid[i,j+1]+previous_grid[i-1,j+1]+previous_grid[i-1,j-1]+previous_grid[i-1,j])/5))\n",
    "    return(final_grid)\n",
    "\n",
    "\"\"\"\n",
    "Function that drops the raws with \"hight 0\" equal to 0\n",
    "\"\"\"\n",
    "\n",
    "def drop_empty_rows(features):\n",
    "    drop_matrix=[]\n",
    "    for i in range(features.shape[0]):\n",
    "        if features.iloc[i]['height 0'] ==0:\n",
    "            drop_matrix.append(i)\n",
    "    features.drop(drop_matrix,0,inplace=True)\n",
    "    return(features)\n",
    "\n",
    "\"\"\"\n",
    "Function that generates final pd.DataFrame with all features\n",
    "\n",
    "Parameters: \n",
    "\n",
    "full_df: Dataframe\n",
    "Dataframe with fourier spectrum\n",
    "\n",
    "n: int\n",
    "number of peaks\n",
    "\n",
    "th1: float, from 0 to 1\n",
    "threshhold\n",
    "\n",
    "th2: float, from 0 to 1\n",
    "threshhold\n",
    "\n",
    "lf_th: float\n",
    "low frequency threshhold\n",
    "\n",
    "path: str\n",
    "path to save the matrix\n",
    "\n",
    "download: bool\n",
    "download or not download feature matrix\n",
    "\n",
    "Returns: \n",
    "\n",
    "features: DataFrame\n",
    "features for full_df dataframe\n",
    "\"\"\"\n",
    "\n",
    "def create_feature_df(list_df, target, flag, n, th1, th2, lf_th, path, name, download=False):\n",
    "    \n",
    "    features = pd.DataFrame()\n",
    "    \n",
    "    properties = properties_of_peaks(list_df, n=n, lf_th=lf_th) \n",
    "    num_peak_1 = number_of_peaks(list_df, th=th1)\n",
    "    num_peak_2 = number_of_peaks(list_df, th=th2)\n",
    "    target.fillna(value=-1, axis=1, inplace=True)\n",
    "    features = pd.concat([features, num_peak_1, num_peak_2, properties], axis=1)\n",
    "    \n",
    "    neighbour_features = pd.DataFrame()\n",
    "    for raw in range(int(np.size(features,0)/64)): #number of basket\n",
    "        features_basket = pd.DataFrame() \n",
    "        for col in range(int(np.size(features,1))): #features\n",
    "            index_set=[] #index matrix for last column indexes\n",
    "            for i in range(raw*64,(raw+1)*64):\n",
    "                index_set.append(i)\n",
    "            resized_set = np.resize(features.iloc[raw*64:(raw+1)*64,col],(8,8)) #8x8 grid for one feature\n",
    "            if resized_set.sum() ==0:\n",
    "                last_column = np.zeros(64)\n",
    "            else:\n",
    "                last_column = calculation_neighbour_features(resized_set)\n",
    "            last_column = pd.DataFrame(last_column, columns = ['neighbour ' + features.columns[col]], index = [index_set])\n",
    "            features_basket = pd.concat([features_basket,last_column], axis=1)\n",
    "        neighbour_features = pd.concat([neighbour_features,features_basket], axis=0)\n",
    "        \n",
    "    features_with_neighbour = pd.concat([s.reset_index(drop=True) for s in [features,neighbour_features,target,flag]], axis=1)\n",
    "    features = pd.concat([s.reset_index(drop=True) for s in [features,target]], axis=1)\n",
    "    \n",
    "    features_with_neighbour = drop_empty_rows(features_with_neighbour)\n",
    "    features = drop_empty_rows(features)\n",
    "    \n",
    "    if download == True: \n",
    "        features_with_neighbour.to_csv(path + name)\n",
    "        return(features_with_neighbour)\n",
    "    else:\n",
    "        return(features_with_neighbour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'D:\\Data\\reannotation\\All annotations with semidrivers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Data\\reannotation\\Annotation with semidrivers HD\\F26.csv (16667, 192)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\pandas\\core\\frame.py:4153: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  downcast=downcast,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flag 1 0\n",
      "flag 2 0\n",
      "flag 3 0\n",
      "(99, 3200) (99, 3200)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|████████████                                                                        | 1/7 [00:32<03:17, 32.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Data\\reannotation\\Annotation with semidrivers HD\\F27.csv (16383, 192)\n",
      "flag 1 64\n",
      "flag 2 64\n",
      "flag 3 64\n",
      "(99, 3072) (99, 3072)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|████████████████████████                                                            | 2/7 [01:06<02:44, 32.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Data\\reannotation\\Annotation with semidrivers HD\\F28.csv (18311, 192)\n",
      "flag 1 128\n",
      "flag 2 128\n",
      "flag 3 128\n",
      "(99, 3584) (99, 3584)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|████████████████████████████████████                                                | 3/7 [01:48<02:23, 35.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Data\\reannotation\\Annotation with semidrivers HD\\F29.csv (18311, 192)\n",
      "flag 1 192\n",
      "flag 2 192\n",
      "flag 3 192\n",
      "(99, 3584) (99, 3584)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|████████████████████████████████████████████████                                    | 4/7 [02:30<01:52, 37.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Data\\reannotation\\Annotation with semidrivers HD\\F30.csv (10173, 192)\n",
      "flag 1 256\n",
      "flag 2 256\n",
      "flag 3 256\n",
      "(99, 1536) (99, 1536)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|████████████████████████████████████████████████████████████                        | 5/7 [02:40<00:58, 29.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Data\\reannotation\\Annotation with semidrivers HD\\F31.csv (10173, 192)\n",
      "flag 1 320\n",
      "flag 2 320\n",
      "flag 3 320\n",
      "(99, 1536) (99, 1536)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|████████████████████████████████████████████████████████████████████████            | 6/7 [02:51<00:23, 23.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Data\\reannotation\\Annotation with semidrivers HD\\F32.csv (10173, 192)\n",
      "flag 1 384\n",
      "flag 2 384\n",
      "flag 3 384\n",
      "(99, 1536) (99, 1536)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 7/7 [03:02<00:00, 26.08s/it]\n"
     ]
    }
   ],
   "source": [
    "MEM_spectra = []\n",
    "NIOM_spectra=[]\n",
    "all_targets_el=pd.DataFrame()\n",
    "all_targets_om=pd.DataFrame()\n",
    "all_flags_el=pd.DataFrame()\n",
    "all_flags_om=pd.DataFrame()\n",
    "final_target_flags=pd.DataFrame()\n",
    "flag_number = 0\n",
    "for filename in tqdm(glob.glob(os.path.join(path, '*.csv'))):\n",
    "    data = pd.read_csv(filename, sep=';',header=0)\n",
    "    print(filename, data.shape)\n",
    "    path, file = os.path.split(filename)\n",
    "    MEM_spectrum,NIOM_spectrum, target_el,target_om, flag_el,flag_om,final_target_flag, flag_number = full_spec_and_freq(data, flag_number)\n",
    "    print(MEM_spectrum.shape,NIOM_spectrum.shape)\n",
    "    MEM_spectra.append(MEM_spectrum)\n",
    "    NIOM_spectra.append(NIOM_spectrum)\n",
    "    all_targets_el = pd.concat([all_targets_el, target_el], axis=0)\n",
    "    all_targets_om = pd.concat([all_targets_om, target_om], axis=0)\n",
    "    all_flags_el = pd.concat([all_flags_el, flag_el], axis=0)\n",
    "    all_flags_om = pd.concat([all_flags_om, flag_om], axis=0)\n",
    "    final_target_flags = pd.concat([final_target_flags, final_target_flag], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "features_el = create_feature_df(MEM_spectra, all_targets_el,all_flags_el, n=5, th1=0.05, th2=0.1, lf_th=1.5, path=path, name = '\\Feature matrix electrode signal AF semidrivers with flags.csv', download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0.0    20747\n",
       " 0.5     3356\n",
       " 1.0     2935\n",
       "-1.0     1634\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_el['target'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
